{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"code","source":"# bbox-utility, check https://github.com/awsaf49/bbox for source code\n!pip install -q /kaggle/input/loguru-lib-ds/loguru-0.5.3-py3-none-any.whl\n!pip install -q /kaggle/input/bbox-lib-ds","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-14T02:07:12.301295Z","iopub.execute_input":"2022-02-14T02:07:12.301614Z","iopub.status.idle":"2022-02-14T02:08:12.408866Z","shell.execute_reply.started":"2022-02-14T02:07:12.301581Z","shell.execute_reply":"2022-02-14T02:08:12.407761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# norfair dependencies\n%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index\n%cd ..","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:08:12.411096Z","iopub.execute_input":"2022-02-14T02:08:12.411391Z","iopub.status.idle":"2022-02-14T02:09:31.249079Z","shell.execute_reply.started":"2022-02-14T02:08:12.411357Z","shell.execute_reply":"2022-02-14T02:09:31.248287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-14T02:09:31.250938Z","iopub.execute_input":"2022-02-14T02:09:31.251193Z","iopub.status.idle":"2022-02-14T02:09:32.637387Z","shell.execute_reply.started":"2022-02-14T02:09:31.251162Z","shell.execute_reply":"2022-02-14T02:09:32.63643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please Upvote if you find this Helpful","metadata":{}},{"cell_type":"markdown","source":"#  Meta Data\n* `train_images/` - Folder containing training set photos of the form `video_{video_id}/{video_frame}.jpg`.\n\n* `[train/test].csv` - Metadata for the images. As with other test files, most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download.\n\n* `video_id` - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n* `video_frame` - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n* `sequence_frame` - The frame number within a given sequence.\n* `image_id` - ID code for the image, in the format `{video_id}-{video_frame}`\n* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate `(x_min, y_min)` of its lower left corner within the image together with its `width` and `height` in pixels --> (COCO format).","metadata":{}},{"cell_type":"markdown","source":"###### ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\n# CKPT_DIR  = '/kaggle/input/greatbarrierreef-yolov5-train-ds'\nCKPT_PATH = '/kaggle/input/leonv5inferv5s60/best.pt' # by @steamedsheep\nIMG_SIZE  = 6400\nCONF      = 0.30\nIOU       = 0.50\nAUGMENT   = True","metadata":{"execution":{"iopub.status.busy":"2022-01-03T06:34:55.27914Z","iopub.execute_input":"2022-01-03T06:34:55.279794Z","iopub.status.idle":"2022-01-03T06:34:55.409887Z","shell.execute_reply.started":"2022-01-03T06:34:55.279751Z","shell.execute_reply":"2022-01-03T06:34:55.409186Z"}}},{"cell_type":"code","source":"# Train Data\ndf = pd.read_csv(f'/kaggle/input/tensorflow-great-barrier-reef/train.csv')\ndf['image_path'] = f'/kaggle/input/tensorflow-great-barrier-reef/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\ndf['annotations'] = df['annotations'].progress_apply(eval)\ndisplay(df.head(2))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:32.639258Z","iopub.execute_input":"2022-02-14T02:09:32.639505Z","iopub.status.idle":"2022-02-14T02:09:33.1378Z","shell.execute_reply.started":"2022-02-14T02:09:32.639478Z","shell.execute_reply":"2022-02-14T02:09:33.136981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of BBoxes","metadata":{}},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts()/len(df)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:33.138915Z","iopub.execute_input":"2022-02-14T02:09:33.139143Z","iopub.status.idle":"2022-02-14T02:09:33.246795Z","shell.execute_reply.started":"2022-02-14T02:09:33.139116Z","shell.execute_reply":"2022-02-14T02:09:33.245891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Helper","metadata":{}},{"cell_type":"code","source":"# check https://github.com/awsaf49/bbox for source code of following utility functions\nfrom bbox.utils import coco2yolo, coco2voc, voc2yolo, voc2coco\nfrom bbox.utils import draw_bboxes, load_image\nfrom bbox.utils import clip_bbox, str2annot, annot2str\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-14T02:09:33.247895Z","iopub.execute_input":"2022-02-14T02:09:33.248109Z","iopub.status.idle":"2022-02-14T02:09:34.006294Z","shell.execute_reply.started":"2022-02-14T02:09:33.248083Z","shell.execute_reply":"2022-02-14T02:09:34.005693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-14T02:09:34.008009Z","iopub.execute_input":"2022-02-14T02:09:34.008603Z","iopub.status.idle":"2022-02-14T02:09:35.457491Z","shell.execute_reply.started":"2022-02-14T02:09:34.008556Z","shell.execute_reply":"2022-02-14T02:09:35.456463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(ckpt_path, conf=0.25, iou=0.50):\n    model = torch.hub.load('/kaggle/input/yolov5-lib-ds',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 1000  # maximum number of detections per image\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-14T02:09:35.458853Z","iopub.execute_input":"2022-02-14T02:09:35.459544Z","iopub.status.idle":"2022-02-14T02:09:35.466501Z","shell.execute_reply.started":"2022-02-14T02:09:35.459506Z","shell.execute_reply":"2022-02-14T02:09:35.465626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"## Helper","metadata":{}},{"cell_type":"code","source":"def predict(model, img, size=768, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-14T02:09:35.46817Z","iopub.execute_input":"2022-02-14T02:09:35.468551Z","iopub.status.idle":"2022-02-14T02:09:35.481783Z","shell.execute_reply.started":"2022-02-14T02:09:35.468519Z","shell.execute_reply":"2022-02-14T02:09:35.48096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tracker","metadata":{}},{"cell_type":"code","source":"from norfair import Detection, Tracker\n\n# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n\n\ndef tracking_function(tracker, frame_id, bboxes, scores):\n    \n    detects = []\n    predictions = []\n    \n    if len(scores)>0:\n        for i in range(len(bboxes)):\n            box = bboxes[i]\n            score = scores[i]\n            x_min = int(box[0])\n            y_min = int(box[1])\n            bbox_width = int(box[2])\n            bbox_height = int(box[3])\n            detects.append([x_min, y_min, x_min+bbox_width, y_min+bbox_height, score])\n            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n#             print(predictions[:-1])\n    # Update tracks using detects from current frame\n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id)) #tracker.updateは検出したオブジェクトを返す。#オプションのdetectionshは処理中の現在のフレームで検出された検出物を表すDetectionのリスト。\n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n\n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:35.48504Z","iopub.execute_input":"2022-02-14T02:09:35.485565Z","iopub.status.idle":"2022-02-14T02:09:36.458678Z","shell.execute_reply.started":"2022-02-14T02:09:35.48553Z","shell.execute_reply":"2022-02-14T02:09:36.45776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Train**","metadata":{}},{"cell_type":"code","source":"CKPT_PATH = '../input/yolov5s6/f2_sub2.pt'\nCONF = 0.30 #0.30\nIOU = 0.50\nIMG_SIZE = 6400\nAUGMENT = False\nAlb=False","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:36.459962Z","iopub.execute_input":"2022-02-14T02:09:36.460201Z","iopub.status.idle":"2022-02-14T02:09:36.465001Z","shell.execute_reply.started":"2022-02-14T02:09:36.46017Z","shell.execute_reply":"2022-02-14T02:09:36.464144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:36.466142Z","iopub.execute_input":"2022-02-14T02:09:36.466401Z","iopub.status.idle":"2022-02-14T02:09:36.483884Z","shell.execute_reply.started":"2022-02-14T02:09:36.466371Z","shell.execute_reply":"2022-02-14T02:09:36.483075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport ast\nimport albumentations as albu","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:36.485386Z","iopub.execute_input":"2022-02-14T02:09:36.485852Z","iopub.status.idle":"2022-02-14T02:09:37.983612Z","shell.execute_reply.started":"2022-02-14T02:09:36.48581Z","shell.execute_reply":"2022-02-14T02:09:37.982656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Trans = albu.Compose([\n    #albu.RandomCrop(width=128, height=72),\n    #albu.HorizontalFlip(p=0.5),\n    #albu.RandomBrightnessContrast(p=1.0),\n    albu.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=0.5),\n    albu.RGBShift (r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, always_apply=False, p=0.5),\n    #albu.Normalize (mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n    ##albu,ChannelDropout (channel_drop_range=(1, 1), fill_value=0, always_apply=False, p=0.5),\n    #albu.ChannelShuffle(p=1.0),\n    #albu.RandomRotate90(p=0.5),\n    #albu.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8),p=1.0), #(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5)\n    #albu.FDA(FDA_reference['image_path'].values,p=0.5),\n    #albu.Affine(p=1.0), #(scale=None, translate_percent=None, translate_px=None, rotate=None, shear=None, interpolation=1, mask_interpolation=0, cval=0, cval_mask=0, mode=0, fit_output=False, always_apply=False, p=0.5)\n    #albu.CenterCrop(128,72,p=0.5), #(height, width, always_apply=False, p=1.0)\n    #albu.RandomCropNearBBox(), #(max_part_shift=(0.3, 0.3), cropping_box_key='cropping_bbox', always_apply=False, p=1.0)\n    #albu.RandomRotate90(p=0.5),\n    #albu.RandomResizedCrop(p=0.5), #(height, width, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1, always_apply=False, p=1.0)\n    #albu.GaussianBlur(), #(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=0.5)\n    #albu.GaussNoise(), #(var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=0.5\n    #albu.Blur(), #(blur_limit=7, always_apply=False, p=0.5)\n    #albu.\n    \n])","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:37.984856Z","iopub.execute_input":"2022-02-14T02:09:37.985127Z","iopub.status.idle":"2022-02-14T02:09:37.992549Z","shell.execute_reply.started":"2022-02-14T02:09:37.985095Z","shell.execute_reply":"2022-02-14T02:09:37.990986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CLAHE(image):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    equalized = clahe.apply(gray)\n    return equalized\ndef Gamma_enhancement(image):\n    gamma = 1/0.6\n    R = 255.0\n    return (R * np.power(image.astype(np.uint32)/R, gamma)).astype(np.uint8)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:37.993668Z","iopub.execute_input":"2022-02-14T02:09:37.994299Z","iopub.status.idle":"2022-02-14T02:09:38.01024Z","shell.execute_reply.started":"2022-02-14T02:09:37.994262Z","shell.execute_reply":"2022-02-14T02:09:38.009376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## repos enhancment","metadata":{}},{"cell_type":"code","source":"def RecoverCLAHE(sceneRadiance):\n    clahe = cv2.createCLAHE(clipLimit=7, tileGridSize=(14, 14))\n    for i in range(3):\n\n        \n        sceneRadiance[:, :, i] = clahe.apply((sceneRadiance[:, :, i]))\n\n\n    return sceneRadiance\n\n#GC\ndef RecoverGC(sceneRadiance):\n    sceneRadiance = sceneRadiance/255.0\n    \n    for i in range(3):\n        sceneRadiance[:, :, i] =  np.power(sceneRadiance[:, :, i] / float(np.max(sceneRadiance[:, :, i])), 3.2)\n    sceneRadiance = np.clip(sceneRadiance*255, 0, 255)\n    sceneRadiance = np.uint8(sceneRadiance)\n    return sceneRadiance","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:38.011721Z","iopub.execute_input":"2022-02-14T02:09:38.012154Z","iopub.status.idle":"2022-02-14T02:09:38.025826Z","shell.execute_reply.started":"2022-02-14T02:09:38.012123Z","shell.execute_reply":"2022-02-14T02:09:38.02505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef global_stretching(img_L,height, width):\n    I_min = np.min(img_L)\n    I_max = np.max(img_L)\n    I_mean = np.mean(img_L)\n\n    array_Global_histogram_stretching_L = np.zeros((height, width))\n    for i in range(0, height):\n        for j in range(0, width):\n            p_out = (img_L[i][j] - I_min) * ((1) / (I_max - I_min))\n            array_Global_histogram_stretching_L[i][j] = p_out\n\n    return array_Global_histogram_stretching_L\n\ndef stretching(img):\n    height = len(img)\n    width = len(img[0])\n    for k in range(0, 3):\n        Max_channel  = np.max(img[:,:,k])\n        Min_channel  = np.min(img[:,:,k])\n        for i in range(height):\n            for j in range(width):\n                img[i,j,k] = (img[i,j,k] - Min_channel) * (255 - 0) / (Max_channel - Min_channel)+ 0\n    return img\n\nfrom skimage.color import rgb2hsv,hsv2rgb\nimport numpy as np\n\n\n\ndef  HSVStretching(sceneRadiance):\n    height = len(sceneRadiance)\n    width = len(sceneRadiance[0])\n    img_hsv = rgb2hsv(sceneRadiance)\n    h, s, v = cv2.split(img_hsv)\n    img_s_stretching = global_stretching(s, height, width)\n\n    img_v_stretching = global_stretching(v, height, width)\n\n    labArray = np.zeros((height, width, 3), 'float64')\n    labArray[:, :, 0] = h\n    labArray[:, :, 1] = img_s_stretching\n    labArray[:, :, 2] = img_v_stretching\n    img_rgb = hsv2rgb(labArray) * 255\n\n    \n\n    return img_rgb\n\ndef sceneRadianceRGB(sceneRadiance):\n\n    sceneRadiance = np.clip(sceneRadiance, 0, 255)\n    sceneRadiance = np.uint8(sceneRadiance)\n\n    return sceneRadiance","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:38.027388Z","iopub.execute_input":"2022-02-14T02:09:38.027796Z","iopub.status.idle":"2022-02-14T02:09:38.041925Z","shell.execute_reply.started":"2022-02-14T02:09:38.027763Z","shell.execute_reply":"2022-02-14T02:09:38.041206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ICM\ndef RecoverICM(img1):\n    img = stretching(img1)\n    sceneRadiance = sceneRadianceRGB(img)\n    sceneRadiance = HSVStretching(sceneRadiance)\n    sceneRadiance = sceneRadianceRGB(sceneRadiance)\n    \n    return sceneRadiance","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:38.043115Z","iopub.execute_input":"2022-02-14T02:09:38.04346Z","iopub.status.idle":"2022-02-14T02:09:38.057267Z","shell.execute_reply.started":"2022-02-14T02:09:38.043339Z","shell.execute_reply":"2022-02-14T02:09:38.056402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_paths = df[df.num_bbox>1].sample(100).image_path.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:38.060293Z","iopub.execute_input":"2022-02-14T02:09:38.061495Z","iopub.status.idle":"2022-02-14T02:09:38.077688Z","shell.execute_reply.started":"2022-02-14T02:09:38.061445Z","shell.execute_reply":"2022-02-14T02:09:38.076949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Debug = True","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:38.081116Z","iopub.execute_input":"2022-02-14T02:09:38.081422Z","iopub.status.idle":"2022-02-14T02:09:38.086426Z","shell.execute_reply.started":"2022-02-14T02:09:38.081387Z","shell.execute_reply":"2022-02-14T02:09:38.085478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Alb=False","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:09:38.088224Z","iopub.execute_input":"2022-02-14T02:09:38.088598Z","iopub.status.idle":"2022-02-14T02:09:38.096915Z","shell.execute_reply.started":"2022-02-14T02:09:38.088556Z","shell.execute_reply":"2022-02-14T02:09:38.096236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Norfair Tracker　の説明","metadata":{}},{"cell_type":"markdown","source":"hit_inertia_min（オプション）：各追跡オブジェクトは、検知にマッチする頻度を追跡する内部ヒットイナーシャカウンターを保持しており、マッチするたびにこのカウンターが上がり、マッチしないたびに下がります。あるフレーム数の間マッチングがなく、この引数で設定された値以下になった場合、オブジェクトは破棄されます。デフォルトは10です。\n\n\nhit_inertia_max (オプション)。各追跡オブジェクトは、検知にマッチする頻度を追跡する内部ヒットイナーシャカウンターを保持しており、マッチするたびにこのカウンターが上がり、マッチしないたびに下がります。この引数は、この慣性力をどれだけ大きくできるかを定義し、したがって、オブジェクトがどの検知にもマッチしない状態でどれだけ長く生きることができるかを定義します。デフォルトは25です。\n\ninitialization_delay (オプション)。各追跡オブジェクトは、その内部ヒット慣性カウンタがhit_inertia_minを超えるまで、トラッカーがユーザーに返す潜在的なオブジェクトとして考慮されるのを待ちます。initialization_delayは、オブジェクトのヒットイナーシャカウンターがhit_inertia_minをどれくらい超えれば、初期化されたとみなされ、実際のオブジェクトとしてユーザーに返されるかを決定する引数です。デフォルトは (hit_inertia_max - hit_inertia_min) / 2 です。\n\n\ndetection_threshold (オプション)。トラッカーに送り込まれる検出ポイントのスコアが、トラッカーに無視されるために、それ以下に落ち込まなければならない閾値を設定します。デフォルトは 0 です。\n\npoint_transience (オプション)。各追跡オブジェクトは、追跡しているポイントがどれくらいの頻度でマッチングしているかを追跡しています。マッチングされているポイントはライブ、そうでないポイントはライブでないと言われています。これは、追跡オブジェクトのどの点が draw_tracked_objects によって描画され、どの点が描画されないかというようなことを決定します。この引数は、マッチングされないポイントがどれだけ短命かを決定します。デフォルトは 4 です。\n\n\nfilter_setup (オプション)。このパラメータは、TrackedObjectインスタンスが使用するKalman Filterのパラメータを変更するために使用します。デフォルトはFilterSetup()です。\n\npast_detections_length: 各追跡オブジェクトについて、保存する過去の検出回数を指定します。Norfairは、オブジェクトのライフタイムを通じて、これらの過去の検出を均一に分配しようとするので、オブジェクトをより代表的にすることができます。各検出に埋め込みを関連付け、距離関数にアクセスできるため、モデルに計量学習を追加したい場合に非常に便利です。デフォルトは4です。","metadata":{}},{"cell_type":"code","source":"np.random.seed(32)\nif Debug:\n    tracker = Tracker(\n        distance_function=euclidean_distance, \n        distance_threshold=30, #30\n        hit_inertia_min=3, #3\n        hit_inertia_max=6, #6\n        initialization_delay=1, #1 追跡対象の各オブジェクトは、内部ヒットインターティアカウンタが、トラッカーによってユーザーに返される可能性のあるオブジェクトとして見なされるまで待機します。この引数は、オブジェクトのヒット慣性カウンタが初期化済みとみなし、実際のオブジェクトとしてユーザーに返されるのにどれくらいの大きな値を超えなければならないかによって決まります。デフォルトは にします。\n        detection_threshold =0, #0 トラッカーに供給される検出のポイントのスコアがトラッカーによって無視されるように下に浸す必要があるしきい値を設定します。デフォルトは にします。0\n        point_transience=4,#4 各追跡対象オブジェクトは、そのトラッキングがどの程度の頻度で一致しているかを追跡します。試合を受けているポイントはライブと言われ、そうでないポイントはライブではないと言われます。これは、トラックされたオブジェクトのどの点がdraw_tracked_objectsによって描画されるか、どの点が描画されないかを決定します。この引数は、一致しない短い一生ポイントがどの程度短いかを決定します。デフォルトは にします。4\n        filter_setup=FilterSetup(R=100, #100 センサ測定ノイズマトリックスの乗数,   自分で変数をいじったりできるやつ。optional): このパラメータを使用して、TrackedObject インスタンスで使用されるカルマン フィルタのパラメータを変更できます。デフォルトはフィルタ設定()です。\n                                 Q=0.1,  #0.1 プロセスの不確実性の乗数\n                                 P=10)   #10 初期共分散行列推定の乗数(位置(速度ではない)変数に対応するエントリのみ\n        #past_detections_length=4 #: 追跡対象のオブジェクトごとに保存する過去の検出の数。Norfairは、オブジェクトの有効期間を通じてこれらの過去の検出を一様に配布しようとします。各検出に埋め込みを関連付け、距離関数でアクセスできるため、モデルにメトリック学習を追加する場合に非常に便利です。デフォルトは にします。4\n        \n        \n    )\n\n    model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n    frame_id = 0\n\n    for idx, path in enumerate(image_paths):\n        img = cv2.imread(path)[...,::-1]\n        if Alb:\n            #img = RecoverCLAHE(img) #意外といい感じ。学習にこれいいかも\n            #img = RecoverGC(img) #ぜんぜんだめ #ごみかす\n            #img = RecoverICM(img) #見た感じはいいが誤検出や検出しないのがある。学習に使えば変わるのかもしれないが時間ないからためせない。\n            img = Trans(image=img)['image']\n            #img = CLAHE(img) #白くなって見落としが増えたからダメ。\n            #img = Gamma_enhancement(image=img) #RecoverICMと同じ感じ\n        bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n        predict_box = tracking_function(tracker, frame_id, bboxes, confis)\n\n        if len(predict_box)>0:\n            box = [list(map(int,box.split(' ')[1:])) for box in predict_box]\n        else:\n            box = []\n        display(show_img(img, box, bbox_format='coco'))\n        #display(show_img(img, bboxes, bbox_format='coco'))\n        if idx>20:\n            break\n        frame_id += 1\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T03:38:05.199943Z","iopub.execute_input":"2022-02-14T03:38:05.200344Z","iopub.status.idle":"2022-02-14T03:38:05.249255Z","shell.execute_reply.started":"2022-02-14T03:38:05.200295Z","shell.execute_reply":"2022-02-14T03:38:05.248456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### past_detections_length","metadata":{}},{"cell_type":"markdown","source":"## Init `Env`","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-14T03:31:32.139522Z","iopub.execute_input":"2022-02-14T03:31:32.139776Z","iopub.status.idle":"2022-02-14T03:31:32.162894Z","shell.execute_reply.started":"2022-02-14T03:31:32.139746Z","shell.execute_reply":"2022-02-14T03:31:32.162272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-02-14T03:31:32.163836Z","iopub.execute_input":"2022-02-14T03:31:32.164592Z","iopub.status.idle":"2022-02-14T03:31:32.168792Z","shell.execute_reply.started":"2022-02-14T03:31:32.164559Z","shell.execute_reply":"2022-02-14T03:31:32.167891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Test**","metadata":{}},{"cell_type":"code","source":"%cd ../working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30, #距離の閾値である #30\n    hit_inertia_min=3, #あるフレーム数の間マッチングがなく、この引数で設定された値以下になった場合、オブジェクトは破棄されます\n    hit_inertia_max=6, #この引数は、この慣性力をどれだけ大きくできるかを定義し、したがって、オブジェクトがどの検知にもマッチしない状態でどれだけ長く生きられるかを定義します。\n    initialization_delay=1,\n)\n\nframe_id = 0\nmodel = load_model(CKPT_PATH, conf=CONF, iou=IOU)\nfor idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    if Alb:\n        #img = Trans(image=img)['image']\n        img = RecoverICM(img)\n        \n    bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    predictions = tracking_function(tracker, frame_id, bboxes, confs)\n    prediction_str = ' '.join(predictions)\n    pred_df['annotations'] = prediction_str\n    env.predict(pred_df)\n    \n    if frame_id < 3:\n        if len(predictions)>0:\n            box = [list(map(int,box.split(' ')[1:])) for box in predictions]\n        else:\n            box = []\n        display(show_img(img, box, bbox_format='coco'))\n    print('Prediction:', pred_df)\n    frame_id += 1","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Check Submission","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"all done\")","metadata":{"execution":{"iopub.status.busy":"2022-02-14T02:28:43.523496Z","iopub.status.idle":"2022-02-14T02:28:43.524059Z","shell.execute_reply.started":"2022-02-14T02:28:43.523862Z","shell.execute_reply":"2022-02-14T02:28:43.523882Z"},"trusted":true},"execution_count":null,"outputs":[]}]}