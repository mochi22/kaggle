(94477, 10, 120) (94477, 10, 63) (94477, 10, 63)
(94477, 10, 246)
<class 'numpy.ndarray'> <class 'numpy.ndarray'>
[(94477, 10, 246), (94477,)]
None
[(77585, 10, 246), (8350, 10, 246), (8542, 10, 246)]
[(77585,), (8350,), (8542,)]
[[[ 4.67521012e-01  3.92403483e-01  1.36448704e-02 ...  4.01093692e-01
    3.91999066e-01 -7.06233382e-02]
  [ 4.65273440e-01  3.96669030e-01  1.64715275e-02 ...  4.38640982e-01
    3.80609751e-01 -5.41826002e-02]
  [ 4.82647777e-01  3.97803724e-01  1.73950493e-02 ...  4.42866981e-01
    3.82043660e-01 -7.98644274e-02]
  ...
  [ 5.11390686e-01  3.96833271e-01  1.53533393e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 5.08696795e-01  3.97768974e-01  1.61361950e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 5.07447839e-01  3.97225887e-01  1.40921492e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]
 [[ 5.20562828e-01  5.23632705e-01 -7.18949456e-03 ...  3.78849626e-01
    4.87079889e-01 -1.05738685e-01]
  [ 5.17362833e-01  5.20749092e-01 -5.66144194e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 5.18603206e-01  5.20824373e-01 -6.37484156e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 4.99292612e-01  5.21909118e-01 -9.16770566e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.91779625e-01  5.18710196e-01 -1.18135093e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.90872860e-01  5.17320335e-01 -8.89495015e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]
 [[ 4.39112067e-01  5.52334905e-01 -6.64041610e-03 ...  1.85525224e-01
    7.41646528e-01 -4.77985665e-02]
  [ 4.44048256e-01  5.47705293e-01 -5.28525561e-03 ...  1.56737328e-01
    7.34309793e-01 -5.76096252e-02]
  [ 4.47527319e-01  5.46965480e-01 -6.43561129e-03 ...  2.42063865e-01
    7.29911327e-01 -5.57842776e-02]
  ...
  [ 4.58184063e-01  5.43513000e-01 -9.24812816e-03 ...  2.14009047e-01
    7.10102499e-01 -5.48707433e-02]
  [ 4.59129304e-01  5.41409612e-01 -6.99981721e-03 ...  2.06549913e-01
    7.01998174e-01 -4.25447188e-02]
  [ 4.62049842e-01  5.41186273e-01 -5.89763839e-03 ...  2.58419394e-01
    7.12684155e-01 -5.34521677e-02]]
 ...
 [[ 4.79570329e-01  3.60501349e-01  2.73153954e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.78081405e-01  3.60182554e-01  1.74500351e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.77429450e-01  3.59742016e-01  1.35115231e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 4.78081405e-01  3.60182554e-01  1.74500351e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.77429450e-01  3.59742016e-01  1.35115231e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.77993906e-01  3.59843016e-01  1.92030752e-03 ...  2.96885997e-01
    6.26876116e-01  1.65453881e-01]]
 [[ 4.15573359e-01  4.94735986e-01  1.66830327e-03 ...  3.70049059e-01
    7.69953489e-01 -8.06361884e-02]
  [ 4.23389286e-01  5.02726376e-01  4.44235513e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.25409764e-01  5.23025095e-01  4.27673850e-03 ...  3.90887499e-01
    8.04830909e-01 -1.09317109e-01]
  ...
  [ 4.30281281e-01  5.14697492e-01  4.25663980e-04 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.24206972e-01  5.11104703e-01  2.18534446e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.22115564e-01  5.25718629e-01  1.48029858e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]
 [[ 3.83232445e-01  3.77185464e-01 -1.57158412e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 3.79777223e-01  3.73707920e-01 -1.63760930e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 3.79922062e-01  3.71459275e-01 -1.61109883e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 3.92555863e-01  3.79994869e-01 -1.57934483e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 3.94882947e-01  3.74345690e-01 -1.45423813e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.04731154e-01  3.72572184e-01 -1.76321659e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]]
train_loader: <torch.utils.data.dataloader.DataLoader object at 0x0000029B39AEBA60> 2425
True
GRU(
  (gru): GRU(246, 512, num_layers=4, batch_first=True)
  (fc): Linear(in_features=512, out_features=250, bias=True)
)
Epoch:0 > Train Loss: 5.9156, Train Acc: 0.0040
Epoch:0 > Val Loss: 5.7958, Val Acc: 0.0046
==================================================
Epoch:1 > Train Loss: 5.7656, Train Acc: 0.0040
Epoch:1 > Val Loss: 5.6929, Val Acc: 0.0044
==================================================
Epoch:2 > Train Loss: 5.6777, Train Acc: 0.0039
Epoch:2 > Val Loss: 5.6324, Val Acc: 0.0043
==================================================
Epoch:3 > Train Loss: 5.6255, Train Acc: 0.0039
Epoch:3 > Val Loss: 5.6063, Val Acc: 0.0032
==================================================
Epoch:4 > Train Loss: 5.5917, Train Acc: 0.0042
Epoch:4 > Val Loss: 5.5738, Val Acc: 0.0044
==================================================
EarlyStopping counter: 1 out of 10
Epoch:5 > Train Loss: 5.5659, Train Acc: 0.0043
Epoch:5 > Val Loss: 5.5544, Val Acc: 0.0043
==================================================
EarlyStopping counter: 2 out of 10
Epoch:6 > Train Loss: 5.5476, Train Acc: 0.0041
Epoch:6 > Val Loss: 5.5405, Val Acc: 0.0046
==================================================
EarlyStopping counter: 3 out of 10
Epoch:7 > Train Loss: 5.5353, Train Acc: 0.0047
Epoch:7 > Val Loss: 5.5341, Val Acc: 0.0035
==================================================
EarlyStopping counter: 4 out of 10
Epoch:8 > Train Loss: 5.5293, Train Acc: 0.0044
Epoch:8 > Val Loss: 5.5280, Val Acc: 0.0048
==================================================
EarlyStopping counter: 5 out of 10
Epoch:9 > Train Loss: 5.5245, Train Acc: 0.0044
Epoch:9 > Val Loss: 5.5236, Val Acc: 0.0043
==================================================
EarlyStopping counter: 6 out of 10
Epoch:10 > Train Loss: 5.5216, Train Acc: 0.0044
Epoch:10 > Val Loss: 5.5227, Val Acc: 0.0036
==================================================
EarlyStopping counter: 7 out of 10
Epoch:11 > Train Loss: 5.5195, Train Acc: 0.0041
Epoch:11 > Val Loss: 5.5209, Val Acc: 0.0043
==================================================
EarlyStopping counter: 8 out of 10
Epoch:12 > Train Loss: 5.5181, Train Acc: 0.0043
Epoch:12 > Val Loss: 5.5199, Val Acc: 0.0041
==================================================
EarlyStopping counter: 9 out of 10
Epoch:13 > Train Loss: 5.5170, Train Acc: 0.0043
Epoch:13 > Val Loss: 5.5197, Val Acc: 0.0031
==================================================
EarlyStopping counter: 10 out of 10
Early Stopping!
best_val_loss: tensor(5.4841, device='cuda:0')
Learning Time: 397.0117793083191
test Loss: 5.5217, test Acc: 0.0041