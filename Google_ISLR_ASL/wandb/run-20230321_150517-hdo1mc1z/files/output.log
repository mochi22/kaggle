(94477, 10, 120) (94477, 10, 63) (94477, 10, 63)
(94477, 10, 246)
<class 'numpy.ndarray'> <class 'numpy.ndarray'>
[(94477, 10, 246), (94477,)]
None
[(77585, 10, 246), (8350, 10, 246), (8542, 10, 246)]
[(77585,), (8350,), (8542,)]
[[[ 4.67521012e-01  3.92403483e-01  1.36448704e-02 ...  4.01093692e-01
    3.91999066e-01 -7.06233382e-02]
  [ 4.65273440e-01  3.96669030e-01  1.64715275e-02 ...  4.38640982e-01
    3.80609751e-01 -5.41826002e-02]
  [ 4.82647777e-01  3.97803724e-01  1.73950493e-02 ...  4.42866981e-01
    3.82043660e-01 -7.98644274e-02]
  ...
  [ 5.11390686e-01  3.96833271e-01  1.53533393e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 5.08696795e-01  3.97768974e-01  1.61361950e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 5.07447839e-01  3.97225887e-01  1.40921492e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]
 [[ 5.20562828e-01  5.23632705e-01 -7.18949456e-03 ...  3.78849626e-01
    4.87079889e-01 -1.05738685e-01]
  [ 5.17362833e-01  5.20749092e-01 -5.66144194e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 5.18603206e-01  5.20824373e-01 -6.37484156e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 4.99292612e-01  5.21909118e-01 -9.16770566e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.91779625e-01  5.18710196e-01 -1.18135093e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.90872860e-01  5.17320335e-01 -8.89495015e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]
 [[ 4.39112067e-01  5.52334905e-01 -6.64041610e-03 ...  1.85525224e-01
    7.41646528e-01 -4.77985665e-02]
  [ 4.44048256e-01  5.47705293e-01 -5.28525561e-03 ...  1.56737328e-01
    7.34309793e-01 -5.76096252e-02]
  [ 4.47527319e-01  5.46965480e-01 -6.43561129e-03 ...  2.42063865e-01
    7.29911327e-01 -5.57842776e-02]
  ...
  [ 4.58184063e-01  5.43513000e-01 -9.24812816e-03 ...  2.14009047e-01
    7.10102499e-01 -5.48707433e-02]
  [ 4.59129304e-01  5.41409612e-01 -6.99981721e-03 ...  2.06549913e-01
    7.01998174e-01 -4.25447188e-02]
  [ 4.62049842e-01  5.41186273e-01 -5.89763839e-03 ...  2.58419394e-01
    7.12684155e-01 -5.34521677e-02]]
 ...
 [[ 4.79570329e-01  3.60501349e-01  2.73153954e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.78081405e-01  3.60182554e-01  1.74500351e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.77429450e-01  3.59742016e-01  1.35115231e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 4.78081405e-01  3.60182554e-01  1.74500351e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.77429450e-01  3.59742016e-01  1.35115231e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.77993906e-01  3.59843016e-01  1.92030752e-03 ...  2.96885997e-01
    6.26876116e-01  1.65453881e-01]]
 [[ 4.15573359e-01  4.94735986e-01  1.66830327e-03 ...  3.70049059e-01
    7.69953489e-01 -8.06361884e-02]
  [ 4.23389286e-01  5.02726376e-01  4.44235513e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.25409764e-01  5.23025095e-01  4.27673850e-03 ...  3.90887499e-01
    8.04830909e-01 -1.09317109e-01]
  ...
  [ 4.30281281e-01  5.14697492e-01  4.25663980e-04 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.24206972e-01  5.11104703e-01  2.18534446e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.22115564e-01  5.25718629e-01  1.48029858e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]
 [[ 3.83232445e-01  3.77185464e-01 -1.57158412e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 3.79777223e-01  3.73707920e-01 -1.63760930e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 3.79922062e-01  3.71459275e-01 -1.61109883e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 3.92555863e-01  3.79994869e-01 -1.57934483e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 3.94882947e-01  3.74345690e-01 -1.45423813e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.04731154e-01  3.72572184e-01 -1.76321659e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]]
train_loader: <torch.utils.data.dataloader.DataLoader object at 0x000001D95846D7B0> 2425
True
GRU(
  (gru): GRU(246, 128, num_layers=2, batch_first=True)
  (fc): Linear(in_features=128, out_features=250, bias=True)
)
Epoch:0 > Train Loss: 4.1424, Train Acc: 0.1277
Epoch:0 > Val Loss: 3.5572, Val Acc: 0.2022
==================================================
Epoch:1 > Train Loss: 2.7373, Train Acc: 0.3437
Epoch:1 > Val Loss: 3.0186, Val Acc: 0.3008
==================================================
Epoch:2 > Train Loss: 2.2443, Train Acc: 0.4497
Epoch:2 > Val Loss: 2.8736, Val Acc: 0.3382
==================================================
Epoch:3 > Train Loss: 1.9468, Train Acc: 0.5210
Epoch:3 > Val Loss: 2.7004, Val Acc: 0.3726
==================================================
Epoch:4 > Train Loss: 1.7574, Train Acc: 0.5655
Epoch:4 > Val Loss: 2.6990, Val Acc: 0.3768
==================================================
EarlyStopping counter: 1 out of 10
Epoch:5 > Train Loss: 1.6278, Train Acc: 0.5995
Epoch:5 > Val Loss: 2.6475, Val Acc: 0.3996
==================================================
Epoch:6 > Train Loss: 1.5435, Train Acc: 0.6218
Epoch:6 > Val Loss: 2.6898, Val Acc: 0.3951
==================================================
EarlyStopping counter: 1 out of 10
Epoch:7 > Train Loss: 1.4860, Train Acc: 0.6381
Epoch:7 > Val Loss: 2.6524, Val Acc: 0.3989
==================================================
Epoch:8 > Train Loss: 1.4470, Train Acc: 0.6489
Epoch:8 > Val Loss: 2.6554, Val Acc: 0.3981
==================================================
Epoch:9 > Train Loss: 1.4207, Train Acc: 0.6561
Epoch:9 > Val Loss: 2.6531, Val Acc: 0.3998
==================================================
Epoch:10 > Train Loss: 1.4033, Train Acc: 0.6614
Epoch:10 > Val Loss: 2.6957, Val Acc: 0.3915
==================================================
EarlyStopping counter: 1 out of 10
Epoch:11 > Train Loss: 1.3914, Train Acc: 0.6644
Epoch:11 > Val Loss: 2.6678, Val Acc: 0.3987
==================================================
EarlyStopping counter: 2 out of 10
Epoch:12 > Train Loss: 1.3833, Train Acc: 0.6666
Epoch:12 > Val Loss: 2.6710, Val Acc: 0.3953
==================================================
Epoch:13 > Train Loss: 1.3779, Train Acc: 0.6688
Epoch:13 > Val Loss: 2.6705, Val Acc: 0.3977
==================================================
Epoch:14 > Train Loss: 1.3745, Train Acc: 0.6699
Epoch:14 > Val Loss: 2.6689, Val Acc: 0.3978
==================================================
Epoch:15 > Train Loss: 1.3718, Train Acc: 0.6704
Epoch:15 > Val Loss: 2.6758, Val Acc: 0.3951
==================================================
EarlyStopping counter: 1 out of 10
Epoch:16 > Train Loss: 1.3702, Train Acc: 0.6711
Epoch:16 > Val Loss: 2.6724, Val Acc: 0.3966
==================================================
EarlyStopping counter: 2 out of 10
Epoch:17 > Train Loss: 1.3692, Train Acc: 0.6715
Epoch:17 > Val Loss: 2.6756, Val Acc: 0.3965
==================================================
EarlyStopping counter: 3 out of 10
Epoch:18 > Train Loss: 1.3685, Train Acc: 0.6717
Epoch:18 > Val Loss: 2.6744, Val Acc: 0.3966
==================================================
EarlyStopping counter: 4 out of 10
Epoch:19 > Train Loss: 1.3679, Train Acc: 0.6719
Epoch:19 > Val Loss: 2.6738, Val Acc: 0.3962
==================================================
Epoch:20 > Train Loss: 1.3677, Train Acc: 0.6721
Epoch:20 > Val Loss: 2.6744, Val Acc: 0.3966
==================================================
EarlyStopping counter: 1 out of 10
Epoch:21 > Train Loss: 1.3675, Train Acc: 0.6719
Epoch:21 > Val Loss: 2.6744, Val Acc: 0.3965
==================================================
EarlyStopping counter: 2 out of 10
Epoch:22 > Train Loss: 1.3675, Train Acc: 0.6721
Epoch:22 > Val Loss: 2.6744, Val Acc: 0.3964
==================================================
EarlyStopping counter: 3 out of 10
Epoch:23 > Train Loss: 1.3673, Train Acc: 0.6720
Epoch:23 > Val Loss: 2.6746, Val Acc: 0.3963
==================================================
EarlyStopping counter: 4 out of 10
Epoch:24 > Train Loss: 1.3672, Train Acc: 0.6721
Epoch:24 > Val Loss: 2.6745, Val Acc: 0.3964
==================================================
EarlyStopping counter: 5 out of 10
Epoch:25 > Train Loss: 1.3672, Train Acc: 0.6722
Epoch:25 > Val Loss: 2.6745, Val Acc: 0.3964
==================================================
EarlyStopping counter: 6 out of 10
Epoch:26 > Train Loss: 1.3671, Train Acc: 0.6722
Epoch:26 > Val Loss: 2.6745, Val Acc: 0.3966
==================================================
EarlyStopping counter: 7 out of 10
Epoch:27 > Train Loss: 1.3672, Train Acc: 0.6722
Epoch:27 > Val Loss: 2.6745, Val Acc: 0.3964
==================================================
EarlyStopping counter: 8 out of 10
Epoch:28 > Train Loss: 1.3672, Train Acc: 0.6722
Epoch:28 > Val Loss: 2.6745, Val Acc: 0.3964
==================================================
EarlyStopping counter: 9 out of 10
Epoch:29 > Train Loss: 1.3671, Train Acc: 0.6722
Epoch:29 > Val Loss: 2.6745, Val Acc: 0.3964
==================================================
EarlyStopping counter: 10 out of 10
Early Stopping!
best_val_loss: tensor(1.5950, device='cuda:0')
Learning Time: 444.96540427207947
test Loss: 2.3062, test Acc: 0.4547