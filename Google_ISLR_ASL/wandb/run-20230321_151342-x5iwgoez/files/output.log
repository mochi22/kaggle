(94477, 10, 120) (94477, 10, 63) (94477, 10, 63)
(94477, 10, 246)
<class 'numpy.ndarray'> <class 'numpy.ndarray'>
[(94477, 10, 246), (94477,)]
None
[(77585, 10, 246), (8350, 10, 246), (8542, 10, 246)]
[(77585,), (8350,), (8542,)]
[[[ 4.67521012e-01  3.92403483e-01  1.36448704e-02 ...  4.01093692e-01
    3.91999066e-01 -7.06233382e-02]
  [ 4.65273440e-01  3.96669030e-01  1.64715275e-02 ...  4.38640982e-01
    3.80609751e-01 -5.41826002e-02]
  [ 4.82647777e-01  3.97803724e-01  1.73950493e-02 ...  4.42866981e-01
    3.82043660e-01 -7.98644274e-02]
  ...
  [ 5.11390686e-01  3.96833271e-01  1.53533393e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 5.08696795e-01  3.97768974e-01  1.61361950e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 5.07447839e-01  3.97225887e-01  1.40921492e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]
 [[ 5.20562828e-01  5.23632705e-01 -7.18949456e-03 ...  3.78849626e-01
    4.87079889e-01 -1.05738685e-01]
  [ 5.17362833e-01  5.20749092e-01 -5.66144194e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 5.18603206e-01  5.20824373e-01 -6.37484156e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 4.99292612e-01  5.21909118e-01 -9.16770566e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.91779625e-01  5.18710196e-01 -1.18135093e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.90872860e-01  5.17320335e-01 -8.89495015e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]
 [[ 4.39112067e-01  5.52334905e-01 -6.64041610e-03 ...  1.85525224e-01
    7.41646528e-01 -4.77985665e-02]
  [ 4.44048256e-01  5.47705293e-01 -5.28525561e-03 ...  1.56737328e-01
    7.34309793e-01 -5.76096252e-02]
  [ 4.47527319e-01  5.46965480e-01 -6.43561129e-03 ...  2.42063865e-01
    7.29911327e-01 -5.57842776e-02]
  ...
  [ 4.58184063e-01  5.43513000e-01 -9.24812816e-03 ...  2.14009047e-01
    7.10102499e-01 -5.48707433e-02]
  [ 4.59129304e-01  5.41409612e-01 -6.99981721e-03 ...  2.06549913e-01
    7.01998174e-01 -4.25447188e-02]
  [ 4.62049842e-01  5.41186273e-01 -5.89763839e-03 ...  2.58419394e-01
    7.12684155e-01 -5.34521677e-02]]
 ...
 [[ 4.79570329e-01  3.60501349e-01  2.73153954e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.78081405e-01  3.60182554e-01  1.74500351e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.77429450e-01  3.59742016e-01  1.35115231e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 4.78081405e-01  3.60182554e-01  1.74500351e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.77429450e-01  3.59742016e-01  1.35115231e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.77993906e-01  3.59843016e-01  1.92030752e-03 ...  2.96885997e-01
    6.26876116e-01  1.65453881e-01]]
 [[ 4.15573359e-01  4.94735986e-01  1.66830327e-03 ...  3.70049059e-01
    7.69953489e-01 -8.06361884e-02]
  [ 4.23389286e-01  5.02726376e-01  4.44235513e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.25409764e-01  5.23025095e-01  4.27673850e-03 ...  3.90887499e-01
    8.04830909e-01 -1.09317109e-01]
  ...
  [ 4.30281281e-01  5.14697492e-01  4.25663980e-04 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.24206972e-01  5.11104703e-01  2.18534446e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.22115564e-01  5.25718629e-01  1.48029858e-03 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]
 [[ 3.83232445e-01  3.77185464e-01 -1.57158412e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 3.79777223e-01  3.73707920e-01 -1.63760930e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 3.79922062e-01  3.71459275e-01 -1.61109883e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  ...
  [ 3.92555863e-01  3.79994869e-01 -1.57934483e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 3.94882947e-01  3.74345690e-01 -1.45423813e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]
  [ 4.04731154e-01  3.72572184e-01 -1.76321659e-02 ...  0.00000000e+00
    0.00000000e+00  0.00000000e+00]]]
train_loader: <torch.utils.data.dataloader.DataLoader object at 0x0000028EF788BAC0> 2425
True
GRU(
  (gru): GRU(246, 256, num_layers=2, batch_first=True)
  (fc): Linear(in_features=256, out_features=250, bias=True)
)
Epoch:0 > Train Loss: 4.7360, Train Acc: 0.0553
Epoch:0 > Val Loss: 4.0933, Val Acc: 0.1098
==================================================
Epoch:1 > Train Loss: 3.4877, Train Acc: 0.2039
Epoch:1 > Val Loss: 3.4852, Val Acc: 0.2159
==================================================
Epoch:2 > Train Loss: 2.8322, Train Acc: 0.3256
Epoch:2 > Val Loss: 3.1691, Val Acc: 0.2774
==================================================
Epoch:3 > Train Loss: 2.4243, Train Acc: 0.4136
Epoch:3 > Val Loss: 3.0776, Val Acc: 0.3006
==================================================
Epoch:4 > Train Loss: 2.1608, Train Acc: 0.4698
Epoch:4 > Val Loss: 2.9656, Val Acc: 0.3247
==================================================
Epoch:5 > Train Loss: 1.9798, Train Acc: 0.5135
Epoch:5 > Val Loss: 2.9482, Val Acc: 0.3313
==================================================
EarlyStopping counter: 1 out of 10
Epoch:6 > Train Loss: 1.8529, Train Acc: 0.5453
Epoch:6 > Val Loss: 2.8983, Val Acc: 0.3456
==================================================
Epoch:7 > Train Loss: 1.7672, Train Acc: 0.5675
Epoch:7 > Val Loss: 2.9329, Val Acc: 0.3460
==================================================
EarlyStopping counter: 1 out of 10
Epoch:8 > Train Loss: 1.7081, Train Acc: 0.5835
Epoch:8 > Val Loss: 2.9052, Val Acc: 0.3480
==================================================
EarlyStopping counter: 2 out of 10
Epoch:9 > Train Loss: 1.6677, Train Acc: 0.5948
Epoch:9 > Val Loss: 2.9078, Val Acc: 0.3477
==================================================
EarlyStopping counter: 3 out of 10
Epoch:10 > Train Loss: 1.6407, Train Acc: 0.6013
Epoch:10 > Val Loss: 2.9024, Val Acc: 0.3509
==================================================
EarlyStopping counter: 4 out of 10
Epoch:11 > Train Loss: 1.6232, Train Acc: 0.6068
Epoch:11 > Val Loss: 2.9046, Val Acc: 0.3508
==================================================
EarlyStopping counter: 5 out of 10
Epoch:12 > Train Loss: 1.6106, Train Acc: 0.6097
Epoch:12 > Val Loss: 2.9069, Val Acc: 0.3496
==================================================
EarlyStopping counter: 6 out of 10
Epoch:13 > Train Loss: 1.6026, Train Acc: 0.6125
Epoch:13 > Val Loss: 2.9096, Val Acc: 0.3503
==================================================
EarlyStopping counter: 7 out of 10
Epoch:14 > Train Loss: 1.5968, Train Acc: 0.6137
Epoch:14 > Val Loss: 2.9075, Val Acc: 0.3502
==================================================
EarlyStopping counter: 8 out of 10
Epoch:15 > Train Loss: 1.5932, Train Acc: 0.6148
Epoch:15 > Val Loss: 2.9039, Val Acc: 0.3515
==================================================
EarlyStopping counter: 9 out of 10
Epoch:16 > Train Loss: 1.5906, Train Acc: 0.6156
Epoch:16 > Val Loss: 2.9058, Val Acc: 0.3508
==================================================
EarlyStopping counter: 10 out of 10
Early Stopping!
best_val_loss: tensor(1.6605, device='cuda:0')
Learning Time: 294.5814492702484
test Loss: 2.5412, test Acc: 0.4017